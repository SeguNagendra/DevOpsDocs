---
sidebar_position: 8
title: Performance & Caching 
---

# Performance & Caching 

Performance tuning is where NGINX truly shines.

Many people install NGINX... Few understand how to tune it properly.

In this module, we cover:

-   worker_processes tuning
-   worker_connections math (real calculation)
-   sendfile
-   keepalive
-   gzip compression
-   proxy_cache explained clearly
-   Cache invalidation thinking
-   Rate limiting deep dive
-   OS-level tuning basics

Layered format:

ðŸŸ¢ Beginner\
ðŸ”µ DevOps Practical\
ðŸ”´ Architect Insight\
âš« Real Production Scenario

------------------------------------------------------------------------

# 1. worker_processes

## ðŸŸ¢ Beginner Understanding

Defines number of worker processes.

Example:

worker_processes auto;

------------------------------------------------------------------------

## ðŸ”µ DevOps Practical View

Best practice:

Set equal to number of CPU cores.

Check cores:

    nproc

Example:

4 CPU cores â†’ worker_processes 4;

------------------------------------------------------------------------

## ðŸ”´ Architect-Level Insight

Each worker handles independent event loop.

Too many workers: Context switching overhead.

Too few workers: CPU underutilized.

Balance based on CPU architecture.

------------------------------------------------------------------------

## âš« Real Production Scenario

8-core server running with worker_processes 1.

CPU underutilized â†’ throughput limited.

Increasing workers improved performance.

------------------------------------------------------------------------

# 2. worker_connections (Real Math)

## ðŸŸ¢ Beginner Understanding

Max connections per worker.

------------------------------------------------------------------------

## ðŸ”µ DevOps Practical View

Example:

worker_processes 4; worker_connections 2048;

Theoretical max connections:

4 Ã— 2048 = 8192

But remember:

Each proxy request may use 2 connections (Client + Upstream).

------------------------------------------------------------------------

## ðŸ”´ Architect-Level Insight

Effective max connections â‰  theoretical value.

Must consider:

-   File descriptor limits
-   Keepalive connections
-   WebSocket connections
-   Upstream connections

Also tune:

worker_rlimit_nofile ulimit -n

------------------------------------------------------------------------

## âš« Real Production Scenario

Traffic spike caused "Too many open files".

Solution:

Increase OS file descriptor limits.

------------------------------------------------------------------------

# 3. sendfile

## ðŸŸ¢ Beginner Understanding

Improves static file serving.

------------------------------------------------------------------------

## ðŸ”µ DevOps Practical View

Enable:

sendfile on;

Allows kernel to send files directly without copying to user space.

------------------------------------------------------------------------

## ðŸ”´ Architect-Level Insight

Reduces CPU usage for static files.

Works best for:

-   Images
-   Videos
-   CSS/JS

------------------------------------------------------------------------

## âš« Real Production Scenario

High static traffic consuming CPU.

Enabling sendfile reduced CPU usage significantly.

------------------------------------------------------------------------

# 4. keepalive_timeout

## ðŸŸ¢ Beginner Understanding

Keeps connection open for reuse.

------------------------------------------------------------------------

## ðŸ”µ DevOps Practical View

Example:

keepalive_timeout 65;

Short timeout â†’ more connection overhead\
Long timeout â†’ more open connections

------------------------------------------------------------------------

## ðŸ”´ Architect-Level Insight

Must balance between:

-   Latency improvement
-   File descriptor usage
-   Memory usage

High traffic systems need careful tuning.

------------------------------------------------------------------------

## âš« Real Production Scenario

Too high keepalive â†’ exhausted file descriptors.

Reduced timeout improved stability.

------------------------------------------------------------------------

# 5. GZIP Compression

## ðŸŸ¢ Beginner Understanding

Compresses response to reduce size.

------------------------------------------------------------------------

## ðŸ”µ DevOps Practical View

Enable:

gzip on; gzip_types text/plain text/css application/json;

Reduces bandwidth usage.

------------------------------------------------------------------------

## ðŸ”´ Architect-Level Insight

Trade-off:

-   CPU cost for compression
-   Bandwidth savings

Beneficial for text content. Not useful for already compressed images.

------------------------------------------------------------------------

## âš« Real Production Scenario

Enabling gzip reduced page load time by 40%.

------------------------------------------------------------------------

# 6. proxy_cache (Important)

## ðŸŸ¢ Beginner Understanding

NGINX stores backend responses temporarily.

Next request served from cache.

------------------------------------------------------------------------

## ðŸ”µ DevOps Practical View

Basic example:
```
proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=mycache:10m
max_size=1g;

location / { proxy_cache mycache; proxy_pass http://backend; }
```
------------------------------------------------------------------------

## ðŸ”´ Architect-Level Insight

Cache design questions:

-   What should be cached?
-   For how long?
-   What should never be cached? (login, personalized pages)

Cache invalidation is hard.

Avoid caching dynamic user-specific content.

------------------------------------------------------------------------

## âš« Real Production Scenario

API traffic reduced 70% after caching common responses.

Backend CPU dropped drastically.

------------------------------------------------------------------------

# 7. Rate Limiting (Deep Dive)

## ðŸŸ¢ Beginner Understanding

Limit number of requests per IP.

------------------------------------------------------------------------

## ðŸ”µ DevOps Practical View

Example:
```
limit_req_zone \$binary_remote_addr zone=api:10m rate=10r/s;

location /api { limit_req zone=api burst=20 nodelay; }
```
------------------------------------------------------------------------

## ðŸ”´ Architect-Level Insight

Rate limiting protects against:

-   Brute force
-   API abuse
-   Simple DDoS

Burst allows short traffic spikes.

Without burst â†’ legitimate users may be blocked.

------------------------------------------------------------------------

## âš« Real Production Scenario

Login endpoint under attack.

Rate limiting stabilized system.

------------------------------------------------------------------------

# 8. OS-Level Tuning Basics

## ðŸŸ¢ Beginner Understanding

System settings affect NGINX performance.

------------------------------------------------------------------------

## ðŸ”µ DevOps Practical View

Tune:

ulimit -n\
net.core.somaxconn\
net.ipv4.ip_local_port_range

------------------------------------------------------------------------

## ðŸ”´ Architect-Level Insight

High concurrency systems require:

-   Increased file descriptor limits
-   Proper TCP tuning
-   Monitoring of TIME_WAIT sockets

NGINX performance depends on kernel tuning.

------------------------------------------------------------------------

## âš« Real Production Scenario

High traffic system limited by kernel parameters, not NGINX config.

After sysctl tuning, performance improved.

------------------------------------------------------------------------

# Summary

After this module, you understand:

âœ” worker_processes tuning\
âœ” worker_connections math\
âœ” sendfile usage\
âœ” keepalive tuning\
âœ” gzip compression\
âœ” proxy_cache basics\
âœ” Rate limiting strategy\
âœ” OS-level tuning awareness

Next: Logging & Observability (Layered).
